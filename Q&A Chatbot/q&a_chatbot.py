# -*- coding: utf-8 -*-
"""Q&A Chatbot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FbpjDelnR0ug4-1fWU1kEUSwKQ0w5Zqk
"""

import tensorflow as tf

tf.test.is_gpu_available()

tf.test.gpu_device_name()

import pickle
import numpy as np

#STEP-1: Install Import Libraries
!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

#STEP-2: Autheticate E-Mail ID

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

#STEP-3: Get File from Drive using file-ID

#2.1 Get the file
downloaded = drive.CreateFile({'id':'12TsfynouQ3E2vagWGVJ2u-Vs-Qav-5Te'})
# replace the id with id of file you want to access
downloaded.GetContentFile('test_qa.txt')

with open("test_qa.txt", "rb") as fp:   # Unpickling
    test_data =  pickle.load(fp)

#STEP-1: Install Import Libraries
!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

#STEP-2: Autheticate E-Mail ID

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

#STEP-3: Get File from Drive using file-ID

#2.1 Get the file
downloaded = drive.CreateFile({'id':'1c4pw-dW9wYLNzY5YbS3NtQebIKm04FHp'})
# replace the id with id of file you want to access
downloaded.GetContentFile('train_qa.txt')

with open("train_qa.txt", "rb") as fp:   # Unpickling
    train_data =  pickle.load(fp)

type(test_data)

len(test_data)

type(train_data)

len(train_data)

train_data[0]

' '.join(train_data[0][0])

# Note the training dataset has space inbetween the period and last word of each sentence
# Similarly for every other punctuation.

' '.join(train_data[0][1])

# Create a set that holds the vocab words
vocab = set()

all_data = test_data + train_data

for story, question , answer in all_data:
    vocab = vocab.union(set(story))
    vocab = vocab.union(set(question))

vocab.add('no')
vocab.add('yes')

vocab

# This is a small vocabulary set by which we're unfortunately forced to select words from it. (Sorry for that. :( )

vocab_len = len(vocab) + 1 #we add an extra space to hold a 0 for Keras's pad_sequences

max_story_len = max([len(data[0]) for data in all_data])

max_story_len

max_question_len = max([len(data[1]) for data in all_data])

max_question_len

# Reserve 0 for pad_sequences
vocab_size = len(vocab) + 1

from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import Tokenizer

# integer encode sequences of words
tokenizer = Tokenizer(filters=[])
tokenizer.fit_on_texts(vocab)

tokenizer.word_index

train_story_text = []
train_question_text = []
train_answers = []

for story,question,answer in train_data:
    train_story_text.append(story)
    train_question_text.append(question)

train_story_seq = tokenizer.texts_to_sequences(train_story_text)

len(train_story_text)

def vectorize_stories(data, word_index=tokenizer.word_index, max_story_len=max_story_len,max_question_len=max_question_len):
    # For this problem
    # X = STORIES
    X = []
    # Xq = QUERY/QUESTION
    Xq = []
    # Y = CORRECT ANSWER
    Y = []
    
    
    for story, query, answer in data:
        
        x = [word_index[word.lower()] for word in story]
        xq = [word_index[word.lower()] for word in query]

        y = np.zeros(len(word_index) + 1)
        
        y[word_index[answer]] = 1
        
        # Append each set of story,query, and answer to their respective holding lists
        X.append(x)
        Xq.append(xq)
        Y.append(y)
        
    # Finally, pad the sequences based on their max length so the RNN can be trained on uniformly long sequences.
        
    # RETURN TUPLE FOR UNPACKING
    return (pad_sequences(X, maxlen=max_story_len),pad_sequences(Xq, maxlen=max_question_len), np.array(Y))

inputs_train, queries_train, answers_train = vectorize_stories(train_data)

inputs_test, queries_test, answers_test = vectorize_stories(test_data)

inputs_test

queries_test

answers_test

sum(answers_test)

# We have count for only 2 columns which are YES and NO.

tokenizer.word_index['yes']

tokenizer.word_index['no']

# Creating model

from keras.models import Sequential, Model
from keras.layers.embeddings import Embedding
from keras.layers import Input, Activation, Dense, Permute, Dropout
from keras.layers import add, dot, concatenate
from keras.layers import LSTM

# Creating placeholders

input_sequence = Input((max_story_len,))
question = Input((max_question_len,))

# Input Encoder M

# Input gets embedded to a sequence of vectors
input_encoder_m = Sequential()
input_encoder_m.add(Embedding(input_dim=vocab_size,output_dim=64))
input_encoder_m.add(Dropout(0.3))

# Output:
# (samples, story_maxlen, embedding_dim)

# Input Encoder C

# embed the input into a sequence of vectors of size query_maxlen
input_encoder_c = Sequential()
input_encoder_c.add(Embedding(input_dim=vocab_size,output_dim=max_question_len))
input_encoder_c.add(Dropout(0.3))
# output: (samples, story_maxlen, query_maxlen)

# Question Encoder

# embed the question into a sequence of vectors
question_encoder = Sequential()
question_encoder.add(Embedding(input_dim=vocab_size,
                               output_dim=64,
                               input_length=max_question_len))
question_encoder.add(Dropout(0.3))
# output: (samples, query_maxlen, embedding_dim)

# Encoding the sequence

# encode input sequence and questions (which are indices)
# to sequences of dense vectors
input_encoded_m = input_encoder_m(input_sequence)
input_encoded_c = input_encoder_c(input_sequence)
question_encoded = question_encoder(question)

# Using dot product to compute the match between first i/p vector sequence and the query

match = dot([input_encoded_m, question_encoded], axes=(2, 2))
match = Activation('softmax')(match)

# Adding this match matrix with the second input vector sequence

response = add([match, input_encoded_c])  
response = Permute((2, 1))(response)

# concatenate the match matrix with the question vector sequence
answer = concatenate([response, question_encoded])

# Reduce with RNN (LSTM)
answer = LSTM(32)(answer)

# Regularization with Dropout
answer = Dropout(0.5)(answer)
answer = Dense(vocab_size)(answer)

# we're outputing a probability distribution over the vocabulary
answer = Activation('softmax')(answer)

# building the final model
model = Model([input_sequence, question], answer)
model.compile(optimizer='rmsprop', loss='categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

# Let's do it
history = model.fit([inputs_train, queries_train], answers_train,batch_size=32,epochs=120,validation_data=([inputs_test, queries_test], answers_test))

# Commented out IPython magic to ensure Python compatibility.
# Plotting the training

import matplotlib.pyplot as plt
# %matplotlib inline
print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# Test data evaluation

pred_results = model.predict(([inputs_test, queries_test]))

story =' '.join(word for word in test_data[0][0])
print("Story: ",story)
query = ' '.join(word for word in test_data[0][1])
print("Question: ",query)
print("True Test Answer from Data is:",test_data[0][2])

#Generate prediction from model
val_max = np.argmax(pred_results[0])

for key, val in tokenizer.word_index.items():
    if val == val_max:
        k = key

print("Predicted answer is: ", k)
print("Probability of certainty was: ", pred_results[0][val_max])

# Let's write our own story
# Let's check the Vocabulary to choose correct words
vocab

# Note the whitespace of the periods as I told you before.
my_story = "John left the kitchen . Sandra dropped the football in the garden ."
my_story.split()

# Note that period is considered as seperate token as in training set. This is the reason for that space.

my_question = "Is the football in the garden ?"

my_question.split()

# Same logic for this question mark too.

mydata = [(my_story.split(),my_question.split(),'yes')]

my_story,my_ques,my_ans = vectorize_stories(mydata)

pred_results = model.predict(([ my_story, my_ques]))

#Generate prediction from model
val_max = np.argmax(pred_results[0])

for key, val in tokenizer.word_index.items():
    if val == val_max:
        k = key

print("Predicted answer is: ", k)
print("Probability of certainty was: ", pred_results[0][val_max])

# Thanks for reading my article. Have a great day.

